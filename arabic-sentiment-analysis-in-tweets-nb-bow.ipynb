{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Sentiment Analysis in tweets using Naive Bayes Machine learning Algorithm and unigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Arabic_tweets_negative_20190413.tsv\n",
      "test_Arabic_tweets_positive_20190413.tsv\n",
      "train_Arabic_tweets_negative_20190413.tsv\n",
      "train_Arabic_tweets_positive_20190413.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import nltk\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.metrics.scores import f_measure, precision, recall\n",
    "import collections\n",
    "\n",
    "\n",
    "# Input data files are available in the \"input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "for filename in os.listdir(\"input\"):\n",
    "    print(filename)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "def load_tsv(data_file, n):\n",
    "    data_features = list()\n",
    "    data = list()\n",
    "    infile = open(data_file, encoding='utf-8')\n",
    "    for line in infile:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        label, text = line.split('\\t')\n",
    "        text_features = process_text(text, n)\n",
    "        if text_features:\n",
    "            data_features += text_features\n",
    "            data.append((text_features, label))\n",
    "    return data, data_features\n",
    "\n",
    "def process_text(text, n=1,\n",
    "                 remove_vowel_marks=False,\n",
    "                 remove_repeated_chars=False,\n",
    "                 ):\n",
    "    clean_text = text\n",
    "    if remove_vowel_marks:\n",
    "        clean_text = remove_diacritics(clean_text)\n",
    "    if remove_repeated_chars:\n",
    "        clean_text = remove_repeating_char(clean_text)\n",
    "\n",
    "    if n == 1:\n",
    "        return clean_text.split()\n",
    "    else:\n",
    "        tokens = clean_text.split()\n",
    "        grams = tokens\n",
    "        for i in range(2, n + 1):\n",
    "            grams += [  ' '.join(g) for g in list(window(tokens, i))  ]\n",
    "        return grams\n",
    "\n",
    "\n",
    "\n",
    "def window(words_seq, n):\n",
    "    \"\"\"Returns a sliding window (of width n) over data from the iterable\"\"\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(words_seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n",
    "\n",
    "def document_features(document, corpus_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in corpus_features:\n",
    "        features['has({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files\n",
      "train file (pos) input/train_Arabic_tweets_positive_20190413.tsv\n",
      "train file (neg) input/train_Arabic_tweets_negative_20190413.tsv\n",
      "test file (pos) input/test_Arabic_tweets_positive_20190413.tsv\n",
      "test file (neg) input/test_Arabic_tweets_negative_20190413.tsv\n"
     ]
    }
   ],
   "source": [
    "pos_train_file = 'input/train_Arabic_tweets_positive_20190413.tsv'\n",
    "neg_train_file = 'input/train_Arabic_tweets_negative_20190413.tsv'\n",
    "\n",
    "pos_test_file = 'input/test_Arabic_tweets_positive_20190413.tsv'\n",
    "neg_test_file = 'input/test_Arabic_tweets_negative_20190413.tsv'\n",
    "print('data files')\n",
    "print('train file (pos)', pos_train_file)\n",
    "print('train file (neg)', neg_train_file)\n",
    "print('test file (pos)', pos_test_file)\n",
    "print('test file (neg)', neg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters (ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters\n",
      "n grams: 1\n"
     ]
    }
   ],
   "source": [
    "print('parameters')\n",
    "n = 1\n",
    "print('n grams:', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading train data .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data ....\n",
      "loading test data ....\n"
     ]
    }
   ],
   "source": [
    "print('loading train data ....')\n",
    "pos_train_data, pos_train_feat = load_tsv(pos_train_file, n)\n",
    "neg_train_data, neg_train_feat = load_tsv(neg_train_file, n)\n",
    "print('loading test data ....')\n",
    "pos_test_data, pos_test_feat = load_tsv(pos_test_file, n)\n",
    "neg_test_data, neg_test_feat = load_tsv(neg_test_file, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data info\n",
      "train data size 47000\n",
      "# of positive 23879\n",
      "# of negative 23121\n"
     ]
    }
   ],
   "source": [
    "print('train data info')\n",
    "train_data = pos_train_data + neg_train_data\n",
    "print('train data size', len(train_data))\n",
    "print('# of positive', len(pos_train_data))\n",
    "print('# of negative', len(neg_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 random tweets .... \n",
      "(['طبيعي', 'كل', 'أجوبة', 'امتحان', 'العربي', 'تتشابه', 'اييشش', 'فييه', 'يا', 'واد', '😑'], 'neg')\n",
      "(['بس', 'هو', 'طعمه', 'احلي', 'من', 'ابو', 'جنيه', '😂😂💖💖'], 'pos')\n",
      "(['غيمه', 'هادئة', 'تمطر', 'كلما', 'اشتد', 'حزنك', 'لتغسل', 'عنك', 'ذلك', 'الحزن', '💓', '#صباح_الخير'], 'pos')\n",
      "(['صباح', 'الورد', '🌷'], 'pos')\n",
      "(['ذا', 'الكفار', 'ما', 'بقى', 'شيء', 'الا', 'سووه', '😐'], 'neg')\n",
      "(['أجمل', 'تعب', 'والله', '😂'], 'pos')\n",
      "(['لو', 'مخلينها', 'بالبيت', 'احسن', 'لها', 'يومهم', 'بيخلونها', 'برا!', 'فيه', 'ناس', 'جحلط', 'وربي', 'وعايشين', 'عيشة', 'الطبقيه', 'بشكل', 'غلط!!'], 'neg')\n",
      "(['حكمة', 'بعد', 'العصر', ':', 'مهما', 'كانت', 'الادلة', 'قوية', 'اجحد', 'يا', 'وحش', '😎'], 'pos')\n",
      "(['عش', 'محاطا', 'بكل', 'الأشياء', 'المنسوبه', 'للرقه', '🌸'], 'pos')\n",
      "(['ربما', 'الحياة', 'لا', 'تعطينا', 'كل', 'ما', 'نريد', 'لكن', 'القناعة', 'تعطينا', 'كل', 'الحياة', '🌸'], 'pos')\n",
      "(['⠀', '⠀', '⠀', '⠀', '⠀', 'إذا', 'عثرت', 'على', 'شخص', 'يتقبل', 'افكارك', 'الغريبة', '،', 'سوف', 'يكون', 'بمثابة', 'كنز', 'وليس', 'شخص', 'فقط', '💎', '⠀', '⠀', '⠀', '⠀', '⠀', '⠀'], 'pos')\n",
      "(['اللهم', 'رب', 'الناس', 'أذهب', 'الباس', 'وآشف', 'أنت', 'الشافي', 'لا', 'شفاء', 'إلا', 'شفاؤك', '..', 'شفاء', 'لا', 'يغادر', 'سقما', '..', 'اللهم', 'اشف', '#ولدي_ذياب', 'اللهم…'], 'neg')\n",
      "(['هما', 'بيوحشونا', 'اووي', 'بس', 'مينفعش', 'نتكلم..!!', '💔'], 'neg')\n",
      "(['•', 'فوائد', '#التبرع', 'بالدم', '💉', '..', '•', '#صحة'], 'neg')\n",
      "(['ما', 'اتوقع', 'هذا', 'بيصير', '🤔'], 'neg')\n",
      "(['♡', '~', '~', '~', 'لا', 'تفقد', 'ما', 'أمتلكته', 'يوما', 'حتى', 'لو', 'كان', 'ذكرى', 'طيبة', 'فليس', 'كل', 'قريب', 'ينال', 'ولا', 'كل', 'بعيد', 'ينسى'], 'pos')\n",
      "(['تصدق', 'صح', '🤔'], 'neg')\n",
      "(['صدقت', 'اليسا', 'حين', 'غنت', '\"مش', 'كل', 'اللي', 'بنحبهم', 'هيكونوا', 'لينا', '\"', '💔'], 'neg')\n",
      "(['أثبتت', 'الآبحاث', 'وأثبت', 'العلماء', '..!!:؟', '.', '.', 'ﺃﻥ', 'استخدام', 'التويتر', 'دون', 'زيارة', 'صفحتي', 'قد', 'يؤدي', 'بك', 'الى', 'فقدآن', 'السعآده…'], 'neg')\n",
      "(['•ابدئي', 'بآيات', 'من', 'القرآن', 'قبل', 'دخول', 'في', 'جو', 'المذاكره', 'لان', 'راح', 'تحسين', 'بطمأنينة', 'وتيسير', 'عجيب☁️', '•', 'حاولي', 'تغرسين', 'في', 'بالك', 'ان', 'اللي…'], 'pos')\n",
      "(['🚨', 'النصر', 'ارسل', 'خطاب', 'لاتحاد', 'القدم', 'يفيد', 'باستبعاد', 'حكام', 'لا', 'يقودون', 'اي', 'لقاء', 'للفريق', 'في', 'الجولات', 'القادمة.', '(', 'الجزيرة', ')'], 'pos')\n",
      "(['عطشان', 'يا', 'برق', 'السما', '💔', 'بموت', 'من', 'طول', 'الغياب', 'بموت', 'من', 'كثر', 'الظما', 'وانت', 'على', 'متن', 'السحاب❤️J❤️'], 'neg')\n",
      "(['حسافه', 'ماتوقعته', 'بيلعب', 'فيها', '،', 'الله', 'يسامحك', 'يامحمد', '💔'], 'neg')\n",
      "(['دائما', 'بكل', 'البلدان', 'دور', 'الاخونجية', '-يختارون', 'أحد', 'اتباعهم', 'ويجعلونه', 'رمز', 'للثورة', 'بالدعم', 'الإعلامي', '-يضعون', 'قناصة', 'ويستهدفون…'], 'neg')\n",
      "(['4.19', '||', 'مقال', '📄', 'تحدثت', 'وسائل', 'الإعلام', 'الكوري', 'عن', 'وصول', 'البوم', 'بانقتان', 'Map', 'of', 'the', 'Soul:', 'Persona', 'للمركز', 'الأول', 'في', 'مخطط…'], 'pos')\n",
      "(['حيينا', 'الكوز', 'جرا', 'واطي', '😂'], 'pos')\n",
      "(['قدرو', 'قلوب', 'القوارير', '🤗'], 'pos')\n",
      "(['لعل', 'الله', 'أخر', 'عنك', 'ما', 'تتمنى', 'لأنه', 'أراد', 'أن', 'يأتيك', 'أجمل', 'مما', 'تتخيل', 'إطمئن', '#صباح_الخير', '🌸'], 'pos')\n",
      "(['حتى', 'انا', 'والله', 'كان', 'لذيذ', '😋'], 'pos')\n",
      "(['يمكن', 'جيل', 'التسعينات', 'تلقى', 'ضربات', 'الثلاثين', 'مسبقا', 'فبطمنك', 'رح', 'نوصله', 'واحنا', 'منتوقع', 'ضربات', 'أقوى', '😅'], 'pos')\n",
      "(['⠀', '⠀', '⠀', '⠀', '⠀', '⠀', '⠀', '⠀', '⠀', 'لا', 'تلوم', 'الخافق', 'ان', 'عشق', 'صوتگ', 'فيه', 'بحة', 'تجيب', 'الويل', 'والعافيه❤️', '#نبض_عاشق', '💕'], 'pos')\n",
      "(['#الاهلي_الهلال', 'اكتب', 'توقعك', 'لنتيجة', 'لقاء', 'الهلال', 'والاهلي', 'تحت', 'التاق', '👇', '#تحدي_اسرع_روقان', 'وادخل', 'في', 'سحب', 'قيمة', 'ايفون', 'X', 'على…'], 'neg')\n",
      "(['يكفيني', 'انگ', 'تراقبني..ويكفيني', 'أنني', 'أشتت', 'نبضگ', '..وأسرق', 'وقتگ', 'وأتعب', 'تفكيرك', '..', '💔', '!!'], 'neg')\n",
      "(['💞', '💔', 'لا', 'الدرب', 'دربي', 'ولا', 'العنوان', 'عنواني', 'ولا', 'البلاد', 'ولا', 'الأوطان', 'أوطاني', 'أنا', 'المغيبة', 'في', 'جب', 'الزمان', 'أنا', 'رجع', 'الصد…'], 'neg')\n",
      "(['إنت', 'غيير', '😭', 'إنت', 'مثل', 'اختيي', 'أطلع', 'عليك', 'الي', 'ما', 'أطلعه', 'على', 'أحد'], 'neg')\n",
      "(['الشيء', 'الوحيد', 'الذي', 'وصلوا', 'فيه', 'للعالمية', 'هو', ':', 'المسيار', '..!', '.', 'ترى', 'كانوا', 'يشجعون', 'ريال', 'مدريد', 'ضد', 'النصر', '🤣'], 'pos')\n",
      "(['مبروك', 'لجميع', 'الأبطال', 'الفائزين', 'من', 'فريق', 'دراج', '،،', 'ويستاهلون', '💐'], 'pos')\n",
      "(['في', 'اقاليم', 'البنغال', 'الزوجه', 'التي', 'لا', 'تسمع', 'كلام', 'زوجها', 'يقومون', 'بحلاقة', 'شعرها', 'وتبقى', 'في', 'بيت', 'اهلها', 'حتئ', 'يظهر', 'شعرها', 'وبعد', 'ثلا…'], 'neg')\n",
      "(['ياشين', 'التعب', 'اللي', 'يصحيني', 'مو', 'النوم', ':('], 'neg')\n",
      "(['حسابات', 'ملگية♛', '🌟🌟', 'مغردون', 'مميزون..', 'لهم', 'حضور', 'أنيق', 'وحروف', 'رآقية', '🌸', '#حسابات_ملكية_تستحق_المتابعة', '👇🏻👇🏻👇🏻👇🏻…'], 'pos')\n",
      "(['📌', 'مواقيت', 'الصلاة', '🕌:', '📍مدينة', '#الرياض', '📆', 'السبت', 'شعبان', 'ه', 'أبريل', 'م', 'الفجر', ':13', 'ص', 'الشروق', ':35', 'ص', 'الظهر', ':…'], 'neg')\n",
      "(['قوتنا', 'بعد', 'الله', 'دائما', 'أنتم🌹💙', 'الف', 'مبرووك', '💙', 'الحمد', 'لله', 'و', 'الشكر', 'له', '🙏'], 'pos')\n",
      "(['يا', 'صبر', 'قل', 'لي', 'هل', 'أنا', 'أيوب', '💔', 'أم', 'أنني', 'في', 'لوعتي', 'يعقوب💔', 'أفنيت', 'دهرا', 'في', 'انتظار', 'أحبتي💔', 'فمتى', 'الحبيب', 'إلى', 'الحبيب', 'يؤو…'], 'neg')\n",
      "(['قمة', 'الاحباط', 'يوم', 'تحسب', 'ان', 'حلقة', 'الثرونز', 'اليوم', 'الليل', 'وتطلع', 'فجر', 'الاثنين', 'ماعليكم', 'من', 'عىد', 'الكورة', 'هذا'], 'neg')\n",
      "(['قال', 'النبي', 'صلى', 'الله', 'عليه', 'وسلم:', 'أولى', 'الناس', 'بي', 'يوم', 'القيامة', 'أكثرهم', 'علي', 'صلاة', '🌹', 'اللهم', 'صل', 'وسلم', 'على', 'نبينا', 'محمد', 'وعلى', 'آل…'], 'pos')\n",
      "(['#اعتبرني_في_حياتك', '#صباح_الخيرᅠ', 'نورالله', 'قلوبكم', 'بذكره', 'وشكره', 'وحسن', 'عبادته.', 'ورزقكم', 'حبه', 'واعانكم', 'على', 'طاعته.', 'واكرمكم', 'بج…'], 'pos')\n",
      "(['مراهقه', 'جسمها', 'حلو', ':', 'كبلز', 'مكوة', 'شيميل', 'كس', 'شذوذ', '😬', 'إبريل', ':19:03', 'مساء'], 'neg')\n",
      "(['ههاي', 'حبوشتي', 'اصلا', 'اني', 'موعد', 'نومتي', 'ساعه', 'واصحى', 'ساعه', '🙈'], 'neg')\n",
      "(['اصوات', 'دعاء', 'وابتهالات', 'تعم', 'ارجاء', 'المعمورة', 'بمناسبة', 'اجتماع', 'نقابة', 'العاملين', 'مع', 'ادارة', 'جامعة', 'الازهر', 'بنية', 'الغاء', 'الامتحانات…'], 'neg')\n",
      "(['يقولون', 'عادي', 'يومين', 'و', 'يسكتون', '😊'], 'pos')\n",
      "(['صلاه', 'الفجر', 'واذكار', 'الصباح', '💜'], 'pos')\n",
      "(['تبغوا', 'حكام', 'وفار', 'خليل', 'جلال', '😂'], 'pos')\n",
      "(['متديش', 'لحد', 'فرصه', 'يشوف', 'ضعفك', 'ادعي', 'القوة', 'وإنت', 'في', 'أشد', 'حاجتك', '👌'], 'pos')\n",
      "(['ام', 'كلثوم', '-', 'الاطلال', '♡'], 'pos')\n",
      "(['🍒', 'اللهم', '🍒', 'أجمع', 'قلوبنا', 'على', 'حبك،', 'واجمع', 'جوارحنا', 'على', 'طاعتك', 'واجمع', 'نفوسنا', 'على', 'خشيتك', 'واجمع', 'أرواحنا', 'في', 'جنتك', 'ولا', 'تحرمنا', 'عفوك…'], 'pos')\n",
      "(['#فك_الشفره', '🌹جديدنا', '🌹', '🌹مجموعة', 'مكياج🌹', '🌸بوكس', 'ارواج', 'سائل', 'ثابت', 'درمكول', 'لون', '🌸ظل', 'هدى', 'بيوتي', 'لون', 'مع', 'اضاءه', 'مع', 'احمر', 'خد…'], 'pos')\n",
      "(['.اللهم', 'أعني', 'على', 'ذكرك', 'وشكرك', 'وحسن', 'عبادتك', '♥'], 'pos')\n",
      "(['هلا', 'والله', 'باللي', 'يتغلو', '😢', 'نوررتي', 'التويتر', 'كلله', 'يا', 'قلبي', 'فقدنا', 'طلتك', '..💕💛'], 'neg')\n",
      "(['س', 'أفكر', 'في', '/', '\\u200bآلأقرب', '\\u200bإلى', 'قلبي', 'قبل', 'آلمنآم', 'ف', 'لعلہ', 'يكون', 'ضيف', 'حلمي', 'و', 'قبل', '\\u200bإغمآضة', 'عينآي', '.', '.', 'س', '\\u200bأرسم', '\\u200bآبتسآم…'], 'pos')\n",
      "(['يارب', 'تخليلي', 'اخواتي', 'وتحفظهم', 'من', 'كل', 'شر', '🙏'], 'pos')\n",
      "(['يارب', 'وإن', 'ضعفت', 'الاجسام', 'فأنت', 'القوي', '؛', 'وإن', 'عجز', 'الأطباء', 'فأنت', 'العظيم', 'لا', 'يعجزك', 'شيء', '،', 'وإن', 'قل', 'الدواء', 'فمنك', 'الشفاء', '،', 'الل…'], 'pos')\n",
      "(['اي', 'والله', 'لو', 'ما', 'الطرد', 'كان', 'على', 'الاقل', 'تعادل', '💔'], 'neg')\n",
      "(['الحوثة', 'او', 'مجلس', 'المطلقات', 'الداعم', 'واحد', 'هم', 'شياطين', 'العرب', '#الامارات', 'فلا', 'تعليق', 'بجد', '.'], 'pos')\n",
      "(['✔️', 'الحلقة', 'التاسعة', 'والعشرون', 'من', 'السيرة', 'النبوية', 'في', 'تغريدات', '💥', 'هجرة', 'النبي', 'ﷺ', 'إلى', 'المدينة…'], 'neg')\n",
      "(['تأمم', 'معاكي', 'ملائكة', 'الرحمن', 'من', 'عالي', 'سماه', '🌹'], 'pos')\n",
      "(['ببلك', 'وقتها', '😋'], 'pos')\n",
      "(['شيخة', 'عرب', 'بنت', 'العرب', 'وبنفسها', 'معتزه', 'طيب', 'ونسب', 'زين', 'وادب', 'واللي', 'تبيه', 'تحوزه', 'من', 'مثلها', 'من', 'قدها', 'واقوى', 'شعور', 'اته…'], 'pos')\n",
      "(['حب', 'الطفولة', 'ماتغيره', 'الأيام', 'وأصدق', 'شعور', 'الحب', '..', 'حب', 'الطفولة', ':('], 'neg')\n",
      "(['4.13🌟~|', 'تحديث', 'حساب', 'بانقتان', 'الرسمي:', '\"أهلا', 'كندا', '#spotifyxbts', '\"'], 'pos')\n",
      "(['محبكم', 'يمر', 'بأزمة', 'قلبية', '💔', 'لا', 'تبخلوا', 'علي', 'بدعواتكم', 'في', 'وقت', 'السحر', '🙏🏻', '#مطر_الرياض', '#دعاء_المطر', '#ساعة_استجابة'], 'neg')\n",
      "(['الوطن', 'شكلة', 'كله', 'صرار', 'الليل', 'وجراد', 'بس', 'لا', 'احد', 'يسمعنا', 'ويقول', 'عذاب', 'بسبب', 'حفلات', 'الترفية', '😂'], 'pos')\n",
      "(['حبيبنا', 'وابونا', 'محجوب', 'شريف', 'حضور', 'في', 'عنان', 'سماء', 'هتافات', 'الوطن', '😭', 'طفلك', 'الخليتو', 'دابك', 'ياما', 'حيعجبك', 'شبابو', 'والبنية', 'ام', 'طرحة', 'كبرت…'], 'neg')\n",
      "(['انت', 'قلت', 'الموضوع', 'كله', 'وبساطة', 'خالص', 'أحسنت', 'يادكتور', '✋'], 'neg')\n",
      "(['🕊', 'ليس', 'كل', 'ما', 'تعرفه', 'وتتمناه', 'هو', 'المناسب', 'لك', '#فأقدار', 'الله', 'فوق', 'مستوى', 'التصورات', 'وأعلى', 'من', 'كل', 'الأمنيات', '❤️', '#هناك', 'أقدار', 'ارتقت…'], 'pos')\n",
      "(['حد', 'بيروح', 'القرية', 'اليوم', '؟', 'الي', 'بيروح', 'مر', 'من', 'صوبي', 'ابا', 'اروح', '😭'], 'neg')\n",
      "(['قلبك', 'عامر', 'بالايمان', 'يا', 'اختاه', '🌚'], 'pos')\n",
      "(['بنشوف', 'مين', 'اللي', 'يشخبط', 'الثاني', 'اليوم', '😏'], 'neg')\n",
      "(['طرزان', 'نفسه', 'م', 'عاشر', 'كميه', 'الحيوانات', 'الي', 'انا', 'عاشرتها', '😕'], 'neg')\n",
      "(['اه', 'حجر', 'والله', 'يبو', 'علي', '😔', 'مش', 'راضي', 'يطرى'], 'neg')\n",
      "(['صح', 'كرف', 'وتعب', 'وقرف', 'ومافي', 'زي', 'أيام', 'المدرسة', 'لكن', 'برضو', 'مرحلة', 'الجامعة', 'ححلوة', 'ولها', 'ذكرياتها', '☹'], 'neg')\n",
      "(['أنا', 'اللي', 'سقفت', 'للقرد', 'وزعلت', 'لما', 'اتنطط🐒', '😅'], 'pos')\n",
      "(['اؤمن', 'أن', 'الاصدقاء', 'يبقون', 'للأبد', '،', 'لذا', 'جميع', 'من', 'ذهبوا', 'لم', 'يكونوا', 'أصدقاء', '💛'], 'pos')\n",
      "(['لا', 'تختار', 'الشخص', 'الجميل', '،', 'اختار', 'الشخص', 'الذي', 'يجعل', '💞', 'حياتك', 'جميلة', '🌸'], 'pos')\n",
      "(['عظمة', 'عظمة', 'عظمة', '💖'], 'pos')\n",
      "(['🌺السلام', 'عليكم', 'ورحمة', 'الله', 'وبركاته', '🌺', 'صباح', 'الخير', 'للأنقياء،', 'وأوفياء', 'الروح،', 'لمن', 'دهس', 'اليأس', 'بقدميه', 'وركض', 'خلف', 'الأمل،', 'والثقة…'], 'neg')\n",
      "(['المشكلة', 'ماما', 'انسانه', 'محترمه', 'وربي', 'عيب', 'هالحركات', 'واعيه', 'كمان', '💔', 'حسبي', 'الله', 'ونعم', 'والوكيل', 'فيها', '💔'], 'neg')\n",
      "(['الوالد', 'الله', 'يحفظه', 'بعرس', 'الخال', 'عبدالله', 'الحبران', 'والخال', 'عبدالرحمن', 'الحبران', 'بالحفر', 'الباطن', 'اسأل', 'الله', 'التوفيق', 'لكم', 'وان', 'يرزقك…'], 'pos')\n",
      "(['الهلال', 'ينتصر', '..', 'لكنه', 'بعيد', 'جدا', 'عن', 'مستواه', '!', 'إهدار', 'غريب', '..', 'لفرص', 'سهلة', 'جدا', '!', 'خط', 'دفاع', 'يعاني', 'كثيرا', '!', 'كنو', '..', 'إمك…'], 'pos')\n",
      "(['مدري', 'مو', 'عادتي', 'اصلا', '😭'], 'neg')\n",
      "(['ها', 'دجله😒', 'شنهو', 'الصار', 'حيل', 'اختلفتي', '😥', 'مو', 'انتي', 'ام', 'الخير', 'معقوله', 'جعتي🤚', '.', '.', '#غرق_العباره_في_الموصل', '#عطوره💕', '#شوشو💕…'], 'neg')\n",
      "(['مش', 'عاارف', '😂'], 'pos')\n",
      "(['مو', 'فرح', 'صدقيني', 'الا', 'عشان', 'نذل', 'ام', 'امهم', 'في', 'حركتهم', 'ذي', '🤣'], 'pos')\n",
      "(['بمناسبة', 'فوز', 'الهلال', '..', '💙', 'سحب', 'على', 'آيفون', 'XR📱', 'رتويت', 'وتابع', '-', 'السحب', 'بعد', 'ساعة', 'موثق', 'بالفديو', '💪'], 'pos')\n",
      "(['ازاى', 'كنتو', 'قريبين', 'اوى', 'مننا', 'وعارفينا', 'اوى', 'كده', 'وبرضو', 'عاملين', 'تجرحو', 'فينا', 'ازاى', 'بنحبكم', 'طول', 'الوقت', 'وانتو', 'تأذونا', 'طول', 'الوقت', '💔'], 'neg')\n",
      "(['📸', 'الآن', 'أداء', 'جيني', 'لسولو', '،', 'روزي', 'قامت', 'بتقديم', 'جيني', 'للحضور', 'بطريقة', 'لطيفة', '💗', '-', 'ابريل', '،'], 'pos')\n",
      "(['كل', 'هالعالم', 'تركناهم', 'عشانك', 'وكل', 'هالدنيا', 'نسيناها', 'سواك♥️'], 'pos')\n",
      "(['وايامك', 'بالخير', 'ان', 'شاء', 'الله', '🌺'], 'pos')\n",
      "(['ما', 'لامس', 'القرآن', 'قلبا', 'ضيقا', 'إلا', 'اتسع', '..', '💙'], 'pos')\n",
      "(['علقوني', 'ب', 'المراسيل', 'وابيات', 'القصيد', 'واستباحو', 'سكة', 'البعد', 'قدهم', 'عايفين', '💔'], 'neg')\n",
      "(['سارقها', 'من', 'احمد', 'السقا'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_size = 100\n",
    "print('{} random tweets .... '.format(sample_size))\n",
    "for s in random.sample(train_data, sample_size):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data info\n",
      "test data size 47000\n",
      "# of positive 5970\n",
      "# of negative 5781\n"
     ]
    }
   ],
   "source": [
    "print('test data info')\n",
    "test_data = pos_test_data + neg_test_data\n",
    "print('test data size', len(train_data))\n",
    "print('# of positive', len(pos_test_data))\n",
    "print('# of negative', len(neg_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging all features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging all features ... \n",
      "len(all_features): 770508\n"
     ]
    }
   ],
   "source": [
    "print('merging all features ... ')\n",
    "all_features = pos_train_feat + neg_train_feat + \\\n",
    "               pos_test_feat + pos_test_feat\n",
    "print('len(all_features):', len(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 sample features ...\n",
      "['لن', 'وانت', 'هاني', 'قيمة', 'لنا', '🌹', 'ماله', 'الناس', 'و', 'كل', 'الدوري', 'خلك', 'العالم', 'الزله', 'جدها', 'اخذتني', '🔴ارسلي', 'مثل', 'ثالث', '-', 'ليه', 'رقيتان', ':-', 'مالي', 'كل', 'الوحيد', 'إنما', 'الحادية', 'خليل', 'المتبعثر', 'لها', 'اسرع', 'كان', 'بمفردي', '/0', 'منها', '♪', 'التاني', 'بعض', 'الكتابة', '!!', 'الاول', 'ي', '.', 'ليبيا:', 'الرجفه', 'ساعة', 'لغايته', '💙', 'عليها', 'والقريب', '🔨', 'توجيه', 'تحاول', 'ومن', 'بالصحة', 'دول', 'لايموتون', '❤', '┅━❀', 'المرتاح', 'بطريقة', 'السلامة!', 'حليب', 'نبض…', 'يأتي', 'انا', 'الل…', 'ينتصر', 'فريقي', '🌳', 'شكرا', '#ساعه_استجابه', 'قلبك', 'وموعدنا', 'إن', 'لأمر', 'محمد', '💔', 'روعه', 'الأساسية', 'طاقتي', 'وأبتعد', 'شهدائن…', 'أصبح', 'يتغيرون', 'انا', '💚', 'ثانية،', '⠀┈┉━◈♔♚♔◈━┅┄', '×', 'مستمر', 'من', 'كنتم', 'صباح', '😏', 'كلمة', 'الخير', 'يازعماء', 'هذول']\n"
     ]
    }
   ],
   "source": [
    "print('{} sample features ...'.format(sample_size))\n",
    "print(random.sample(all_features, sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_count = {}\n",
    "for w in all_features:\n",
    "    all_features_count[w] = all_features_count.get(w, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample frequencies\n",
      "[('هذولي', 5), ('تغننا', 1), ('🌴وبارك', 1), ('️⃣يعطى..؟!!', 1), ('هنالك', 9), ('الضروري', 3), ('أنبضها', 2), ('اتكرمت', 1), ('مرتين،', 1), ('❀', 154), ('للوضع', 1), ('غابه', 2), ('ﻳﺮ', 1), ('🙃🙄', 1), ('أحياء..!', 1), ('ولاخلصوو', 2), ('الدورعلى', 1), ('#يابو_دالين❤️', 1), ('سبقه', 1), ('أههخ', 1), ('والقديم', 1), ('بتسقط', 1), ('إنعواجه', 2), ('وزودوها', 1), ('أوك', 1), ('ماتصحى', 2), ('👱🏻\\u200d♀️:', 3), ('شهرا', 2), ('😁🙊', 1), ('ونسى', 1)]\n",
      "freq of word في is 9550\n",
      "freq of word فى is 220\n",
      "freq of word من is 12655\n"
     ]
    }
   ],
   "source": [
    "print('sample frequencies')\n",
    "print(random.sample(list(all_features_count.items()), 30))\n",
    "word = 'في'\n",
    "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
    "word = 'فى'\n",
    "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
    "word = 'من'\n",
    "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data: 47000\n",
      "min document frequency: 47\n",
      "max document frequency: 46060\n"
     ]
    }
   ],
   "source": [
    "print('size of training data:',  len(train_data))\n",
    "min_df = int(0.001 * len(train_data))\n",
    "max_df = int(0.98 * len(train_data))\n",
    "print('min document frequency:', min_df)\n",
    "print('max document frequency:', max_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961 are kept out of 770508\n"
     ]
    }
   ],
   "source": [
    "# remove features that have frequency below/above the threshold\n",
    "my_features = set([word for word, freq in all_features_count.items() if  max_df > freq > min_df ])\n",
    "print(len(my_features), 'are kept out of', len(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample of selected features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 sample of selected features:\n",
      "['اليوتيوب', 'نورا', '[', 'ليله', 'الجاهل', '😷', 'نسألك', '🙄', 'تكون', 'بصوت', 'الدولية', 'وهي', 'بستحق', 'X', 'القمر', 'بالهبوط.', 'أخذ', 'ادري', 'شوي', 'وأنتم', 'أنا', '🌟', 'حرفيا', 'وشولوه', 'اغنية', 'وحروف', 'انتظرك', 'الدهر', 'للحين', 'نعم', 'الرسول', '👌', 'Luv', 'لكنه', 'حينما', 'بسم', 'الحي', 'الصدارة', 'وصلت', 'مبروك', 'باليوم', 'وياك', 'واليوم', '😀', 'وبعدين', 'الفيفا', 'شهور', 'ينام', 'بحاجة', '⠀⠀', 'البشير', 'فين', 'إنت', 'سنوات', 'حياتك', 'توقعك', 'بينك', 'اشهر', 'أين', 'عشان', 'بدي', 'منها', 'هكذا', 'وجه', 'العالم', 'الاول', 'يحدث', 'الفترة', 'وربي', 'ملگية♛', 'سؤال', 'مكان', 'صبح', 'المسيار', 'البعض', 'وتلفظ', 'أمي', 'عشق', 'نتيجة', 'التغريده', 'أشياء', 'انت', 'اما', 'الدوري', '⇣', '😇', 'مثلا', 'شي', 'الحديث', 'نفسه', 'ذمتك', 'أحسن', 'ابد', 'خبر', 'اكثر', 'عرفت', 'الحزن', 'دقيقة', 'نبينا', 'رد']\n"
     ]
    }
   ],
   "source": [
    "print('{} sample of selected features:'.format(sample_size))\n",
    "print(random.sample(list(my_features), sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating features for training documents ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(document_features(d, my_features), c) for (d, c) in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is done\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(feature_sets)\n",
    "print('training is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most informative features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               has(موثق) = True              pos : neg    =    238.5 : 1.0\n",
      "                  has(😭) = True              neg : pos    =    202.0 : 1.0\n",
      "                  has(😢) = True              neg : pos    =    171.3 : 1.0\n",
      "            has(المسيار) = True              pos : neg    =    170.1 : 1.0\n",
      "              has(وصلوا) = True              pos : neg    =    166.9 : 1.0\n",
      "                  has(😳) = True              neg : pos    =    164.2 : 1.0\n",
      "             has(الشروط) = True              pos : neg    =    151.4 : 1.0\n",
      "              has(وتابع) = True              pos : neg    =    143.9 : 1.0\n",
      "               has(ببكي) = True              neg : pos    =    143.6 : 1.0\n",
      "                  has(🥀) = True              neg : pos    =    132.4 : 1.0\n",
      "              has(السحب) = True              pos : neg    =    118.4 : 1.0\n",
      "                  has(💐) = True              pos : neg    =    116.5 : 1.0\n",
      "             has(العروس) = True              neg : pos    =    113.3 : 1.0\n",
      "             has(يشجعون) = True              pos : neg    =    100.5 : 1.0\n",
      "               has(ركبت) = True              neg : pos    =    100.2 : 1.0\n",
      "                has(بضل) = True              neg : pos    =     98.8 : 1.0\n",
      "               has(نشبة) = True              neg : pos    =     97.4 : 1.0\n",
      "                  has(😞) = True              neg : pos    =     97.3 : 1.0\n",
      "                  has(🙈) = True              neg : pos    =     97.2 : 1.0\n",
      "             has(وكأنهم) = True              neg : pos    =     94.7 : 1.0\n",
      "                  has(😐) = True              neg : pos    =     91.9 : 1.0\n",
      "                  has(😔) = True              neg : pos    =     91.3 : 1.0\n",
      "                 has(:() = True              neg : pos    =     87.5 : 1.0\n",
      "                  has(😷) = True              neg : pos    =     85.0 : 1.0\n",
      "              has(شديدة) = True              neg : pos    =     82.4 : 1.0\n",
      "            has(الزرقاء) = True              pos : neg    =     82.3 : 1.0\n",
      "              has(توجيه) = True              neg : pos    =     79.5 : 1.0\n",
      "              has(سودان) = True              neg : pos    =     78.8 : 1.0\n",
      "                 has(فض) = True              neg : pos    =     78.1 : 1.0\n",
      "            has(بمناسبة) = True              pos : neg    =     74.9 : 1.0\n",
      "                  has(😱) = True              neg : pos    =     71.3 : 1.0\n",
      "               has(انظر) = True              pos : neg    =     70.7 : 1.0\n",
      "                  has(🤔) = True              neg : pos    =     67.5 : 1.0\n",
      "              has(قيمته) = True              pos : neg    =     67.5 : 1.0\n",
      "                  has(😒) = True              neg : pos    =     66.7 : 1.0\n",
      "                  has(😕) = True              neg : pos    =     66.0 : 1.0\n",
      "            has(الايفون) = True              pos : neg    =     62.9 : 1.0\n",
      "               has(مبلغ) = True              pos : neg    =     62.1 : 1.0\n",
      "           has(المسابقة) = True              pos : neg    =     61.6 : 1.0\n",
      "                  has(💗) = True              pos : neg    =     61.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating features for test documents ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [(document_features(d, my_features), c) for (d, c) in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classify test instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sets = collections.defaultdict(set)\n",
    "test_sets = collections.defaultdict(set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_features):\n",
    "    ref_sets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    test_sets[observed].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8913283975831844\n",
      "pos precision:  0.9198425478618716\n",
      "pos recall: 0.8611390284757119\n",
      "neg precision:  0.8654657578708211\n",
      "neg recall: 0.9225047569624633\n",
      "positive f-score: 0.8895233151656718\n",
      "negative f-score: 0.8930754416813196\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', nltk.classify.accuracy(classifier, test_features))\n",
    "print('pos precision: ', precision(ref_sets['pos'], test_sets['pos']))\n",
    "print('pos recall:', recall(ref_sets['pos'], test_sets['pos']))\n",
    "print('neg precision: ', precision(ref_sets['neg'], test_sets['neg']))\n",
    "print('neg recall:', recall(ref_sets['neg'], test_sets['neg']))\n",
    "print('positive f-score:', f_measure(ref_sets['pos'], test_sets['pos']))\n",
    "print('negative f-score:', f_measure(ref_sets['neg'], test_sets['neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
